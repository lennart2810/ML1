{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e911ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import pprint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar10\n",
    "from cv2 import cvtColor, COLOR_RGB2GRAY\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from scipy.interpolate import make_interp_spline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d02ac6",
   "metadata": {},
   "source": [
    "# preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad6e7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    \n",
    "    # load cifar10 dataset\n",
    "    (train_x, train_y), (test_x, test_y) = cifar10.load_data()\n",
    "    \n",
    "    # store all numeric labels included in dataset into list\n",
    "    class_labels = list(np.unique(train_y))\n",
    "    \n",
    "    # save class descriptions and merge them with numeric labels to dictonary\n",
    "    class_names = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck'] \n",
    "    class_dict = dict(zip(class_labels, class_names))\n",
    "    \n",
    "    return [train_x, train_y], [test_x, test_y], class_dict\n",
    "\n",
    "\n",
    "def process_data(data):\n",
    "    \n",
    "    # extract images and labels from data list\n",
    "    images, labels = data[0], data[1]\n",
    "    \n",
    "    # convert images to grayscale via list comprehension and cv2 (32x32x3 --> 32x32x1)\n",
    "    images_gray = np.asarray([cvtColor(img, COLOR_RGB2GRAY) for img in images])\n",
    "    \n",
    "    # scale image data (0-255 --> 0-1)\n",
    "    images_scaled = images_gray/255.0\n",
    "    \n",
    "    # flatten image data (32x32x1 --> 1024)\n",
    "    images_flat = images_scaled.reshape(len(images), -1)\n",
    "    \n",
    "    # flatten label data (N,1 --> N,) with N = len(labels)\n",
    "    labels_flat = labels.reshape(-1)\n",
    "    \n",
    "    return [images_flat, labels_flat]\n",
    "\n",
    "\n",
    "def split_train_data(data, proportion):\n",
    "    \n",
    "    # check if proportion is between 0 and 1\n",
    "    assert 0.0 < proportion < 1.0, f'Invalid proportion: {proportion}. It must be between 0 and 1'\n",
    "    \n",
    "    # extract images and labels from data list\n",
    "    images, labels = data[0], data[1]\n",
    "    \n",
    "    # divide the dataset into the given overlap proportion\n",
    "    n = int(len(images)*proportion)\n",
    "    overlap_x, overlap_y = images[:n], labels[:n]\n",
    "    unique_x, unique_y = images[n:], labels[n:]\n",
    "    \n",
    "    return [unique_x, unique_y], [overlap_x, overlap_y]\n",
    "\n",
    "\n",
    "def get_processed_data(proportion:float=0.2, debug:bool=True):\n",
    "    \n",
    "    # load dataset and get class dictonary\n",
    "    train, test, class_dict = load_dataset()\n",
    "    \n",
    "    if debug:\n",
    "        print('1. load CIFAR-10 dataset:\\n')\n",
    "        print(f'train:\\t{train[0].shape}, {train[1].shape}')\n",
    "        print(f'test:\\t{test[0].shape}, {test[1].shape}\\n')\n",
    "        pprint.PrettyPrinter(depth=1).pprint(class_dict)\n",
    "    \n",
    "    # process train and test data\n",
    "    train = process_data(train)\n",
    "    test = process_data(test)\n",
    "    \n",
    "    if debug:\n",
    "        print('\\n2. preprocess dataset:\\n')\n",
    "        print(f'train:\\t{train[0].shape}, {train[1].shape}')\n",
    "        print(f'test:\\t{test[0].shape}, {test[0].shape}')\n",
    "        \n",
    "    # split train data into \"unique\" and \"overlapping\" subsets\n",
    "    unique, overlap = split_train_data(train, proportion)\n",
    "    \n",
    "    if debug:\n",
    "        print('\\n3. prepare train data for ensemble:\\n')\n",
    "        print(f'unique:\\t{unique[0].shape}, {unique[1].shape}')\n",
    "        print(f'overlap:{overlap[0].shape}, {overlap[1].shape}')\n",
    "    \n",
    "    return train, unique, overlap, test, class_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a87cb76",
   "metadata": {},
   "source": [
    "# ensemble class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258c18d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyEnsembleClassifier:\n",
    "    \n",
    "    def __init__(self, base_classifier, n_classifier, class_dict):\n",
    "        \n",
    "        self.clf_n = n_classifier\n",
    "        \n",
    "        self.base_clf = base_classifier # fuction that returns a base classifier\n",
    "        \n",
    "        # initialize list of n base classifier via list comprehension \n",
    "        self.clf = [self.base_clf() for _ in range(self.clf_n)]\n",
    "        print(f'initialized ensemble with {self.clf_n} {self.base_clf()}')\n",
    "        \n",
    "        # store information about data set\n",
    "        self.class_dict = class_dict\n",
    "        self.class_y = list(self.class_dict.keys())\n",
    "        self.class_n = len(self.class_y)\n",
    "        \n",
    "        \n",
    "    def create_subsets(self, unique, overlap, overlap_dict):\n",
    "        \n",
    "        self.subsets = []\n",
    "        \n",
    "        # extract images and labels from unique / overlapping dataset\n",
    "        unique_x, unique_y = unique[0], unique[1]\n",
    "        overlap_x, overlap_y = overlap[0], overlap[1]\n",
    "        \n",
    "        # Len Of Subset, needed for sequential sampling of unique dataset\n",
    "        los = int(len(unique_x)/self.clf_n) \n",
    "        \n",
    "        # create a subset for each classifier \n",
    "        for n in range(self.clf_n):\n",
    "            \n",
    "            # lower and upper boundaries for slicing the dataset\n",
    "            l = n*los; u = (n+1)*los\n",
    "            self.subsets.append([unique_x[l:u], unique_y[l:u]])\n",
    "            \n",
    "            # extract Random Samples from overlapping dataset (low, high, number of samples)\n",
    "            rs = np.random.randint(0, len(overlap_x), overlap_dict[self.clf_n]) \n",
    "            \n",
    "            # append overlapping data to subset list\n",
    "            self.subsets[n][0] = np.append(self.subsets[n][0], overlap_x[rs]).reshape(-1, unique_x[0].shape[0])\n",
    "            self.subsets[n][1] = np.append(self.subsets[n][1], overlap_y[rs])\n",
    "  \n",
    "        print(f'created {self.clf_n} subsets, each containing the following:\\n')\n",
    "        print(f'images:\\t{self.subsets[0][0].shape}')\n",
    "        print(f'labels:\\t{self.subsets[0][1].shape}')\n",
    "    \n",
    "    \n",
    "    def train(self, proportion=1):\n",
    "        \n",
    "        print(f'train each classifier on their corresponding subset:\\n')\n",
    "        \n",
    "        start = time.time()\n",
    "        \n",
    "        # train each classifier on their corresponding subset\n",
    "        for n, clf in enumerate(self.clf):\n",
    "            \n",
    "            print(f'{n+1}. classifier')\n",
    "            \n",
    "            # proportion below 1 reduce the size of the dataset, \n",
    "            # primarily used during development to reduce training time\n",
    "            if proportion < 1:\n",
    "                train_samples = int(len(self.subsets[n][0])*proportion) \n",
    "                train_data_x = self.subsets[n][0][:train_samples]\n",
    "                train_data_y = self.subsets[n][1][:train_samples]\n",
    "                \n",
    "            else:\n",
    "                train_data_x = self.subsets[n][0]\n",
    "                train_data_y = self.subsets[n][1]\n",
    "                \n",
    "            clf.fit(train_data_x, train_data_y)\n",
    "        \n",
    "        self.train_duration = round(time.time() - start, 2)\n",
    "        print(f'\\ntraining duration: {self.train_duration} s')\n",
    "        \n",
    "        \n",
    "    def get_accuracy_score(self, images, labels):\n",
    "            \n",
    "        # get accuracy score for each member\n",
    "        clf_scores = [clf.score(images, labels) for clf in self.clf]\n",
    "\n",
    "        # mean all classifier scores to an ensemble score\n",
    "        return np.asarray(clf_scores).mean()\n",
    "    \n",
    "    \n",
    "    def get_predictions(self, images, labels):\n",
    "\n",
    "        # iterate over all classifier to get their predictions for images (via nested list comprehensions)\n",
    "        predictions = [clf.predict(images) for clf in self.clf]\n",
    "\n",
    "        # convert and reshape predictions (N_images, N_classifier)\n",
    "        return np.asarray(predictions).T\n",
    "        \n",
    "        \n",
    "    def get_relative_predictions(self, predictions):\n",
    "        \n",
    "        # iterate over all predictions to get relative number for all labels (via nested list comprehensions)\n",
    "        relative_predictions = [[np.count_nonzero(pred == y)/self.clf_n for y in self.class_y] for pred in predictions]\n",
    "        \n",
    "        # convert and reshape predictions (N_predictions = N_images, N_classifier)\n",
    "        return np.asarray(relative_predictions).reshape(-1, self.class_n)\n",
    "        \n",
    "        \n",
    "    def get_overall_predictions(self, relative_predictions):\n",
    "        \n",
    "        # extract the index, respectively label, of highest value <-- ensemble prediction for image_i\n",
    "        ensemble_predictions = np.asarray([np.argmax(pred) for pred in relative_predictions])\n",
    "        \n",
    "        # get prediction confidence of the ensemble for image_i\n",
    "        ensemble_prediction_confidences =  np.asarray([np.max(pred) for pred in relative_predictions])\n",
    "        \n",
    "        return ensemble_predictions, ensemble_prediction_confidences\n",
    "        \n",
    "    \n",
    "    def evaluate(self, images, labels):\n",
    "        \n",
    "        # call methods to get needed information for ensemble evaluation\n",
    "        acc = self.get_accuracy_score(images, labels)\n",
    "        pred = self.get_predictions(images, labels)\n",
    "        relative_pred = self.get_relative_predictions(pred)\n",
    "        ensemble_pred, ensemble_pred_confi = self.get_overall_predictions(relative_pred)\n",
    "        \n",
    "        return acc, pred, relative_pred, ensemble_pred, ensemble_pred_confi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7749740",
   "metadata": {},
   "source": [
    "# plotting functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e49697",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['axes.edgecolor'] = '#808080'\n",
    "\n",
    "def interpolate_data(x, y):\n",
    "    \n",
    "    # fit a cubic spline to data\n",
    "    x_new = np.linspace(x.min(), x.max(), 300)\n",
    "    spline = make_interp_spline(x, y, k=3)\n",
    "    y_new = spline(x_new)\n",
    "    \n",
    "    return [x_new, y_new]\n",
    "\n",
    "\n",
    "def plot_MRF(data, title):\n",
    "    \n",
    "    data = np.asarray(data)\n",
    "    np.savetxt(f'./results/raw_data/{title}.csv', data, delimiter=',')\n",
    "    \n",
    "    # preprocess data for plotting\n",
    "    M, R, F, x = data[:,0], data[:,1], data[:,2], data[:,3]\n",
    "    R, F = R*M*100, F*M*100\n",
    "    M = M*100\n",
    "    \n",
    "    M_x, M_y = interpolate_data(x, M)\n",
    "    R_x, R_y = interpolate_data(x, R)\n",
    "    F_x, F_y = interpolate_data(x, F)\n",
    "    \n",
    "    plt.figure(figsize=(12,6))\n",
    "    plt.plot(x, M, 'bo--', linewidth=2, label='$M$') \n",
    "    plt.plot(x, R, 'go--', linewidth=2, label='$R$')\n",
    "    plt.plot(x, F, 'ro--', linewidth=2, label='$F$')\n",
    "    plt.plot(M_x, M_y, 'b', linewidth=5, alpha=0.5)\n",
    "    plt.plot(R_x, R_y, 'g', linewidth=5, alpha=0.5)\n",
    "    plt.plot(F_x, F_y, 'r', linewidth=5, alpha=0.5)\n",
    "    \n",
    "    plt.xlim((-0.05, 1.05))\n",
    "    plt.ylim((-5, 105))\n",
    "    plt.xticks(np.arange(0, 1.1, 0.1))\n",
    "    plt.yticks(np.arange(0, 110, 25))\n",
    "\n",
    "    plt.title(f'{title}', fontsize=20)\n",
    "    plt.xlabel('threshold', fontsize=16)\n",
    "    plt.ylabel('classified images [%]', fontsize=16)\n",
    "    plt.legend(fontsize=16)\n",
    "    \n",
    "    plt.grid()\n",
    "    plt.savefig(f'./results/{title}.png', dpi=300)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144e8bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_evaluation_process(predictions, certainties, label, overall_pred, class_dict, treshold, ax1, ax2):\n",
    "    \n",
    "    n_clf = len(predictions)\n",
    "\n",
    "    # plot classifiers predictions for image_i\n",
    "    color = 'green' if overall_pred == label else 'red'\n",
    "    colors = [color if pred == overall_pred else 'lightgrey' for pred in predictions]\n",
    "    ax1.bar(np.arange(1, n_clf+1), predictions+1, color=colors)\n",
    "    ax1.axhline(label+1, color='black', linestyle='--', alpha=0.3, label=f'true label: {class_dict[label]}')\n",
    "    ax1.set_xlabel('classifier', fontsize=14)\n",
    "    ax1.set_ylabel('labels', fontsize=16)\n",
    "    x_ticks = np.arange(0, n_clf+5, 5)\n",
    "    x_ticks[0] = 1\n",
    "    ax1.set_xticks(x_ticks)\n",
    "    ax1.set_yticks(np.arange(len(class_dict)+2))\n",
    "    y_labels = list(class_dict.keys())\n",
    "    y_labels.insert(0, ''); y_labels.append('')\n",
    "    ax1.set_yticklabels(y_labels)\n",
    "    ax1.legend(loc='lower right', bbox_to_anchor=(1, 1), fontsize=10)\n",
    "\n",
    "    # plot prediction certainties    \n",
    "    max_idx = np.argmax(certainties)\n",
    "    color = 'blue' if np.max(certainties) < treshold else 'black'\n",
    "    colors = [color if i == max_idx else 'lightgrey' for i in range(n_clf)]\n",
    "    ax2.bar(np.arange(10), certainties, color=colors)\n",
    "    ax2.axhline(treshold, color='black', linestyle='--', alpha=0.3, label='treshold')\n",
    "    ax2.set_xlabel('labels', fontsize=14)\n",
    "    ax2.set_ylabel('ensemble confidence [%]', fontsize=16)\n",
    "    ax2.set_xticks(np.arange(10))\n",
    "    ax2.set_yticks(np.arange(0, 1.05, 0.1))\n",
    "    ax2.set_ylim((0, 1.05))\n",
    "    ax2.set_xticklabels(list(class_dict.values()))\n",
    "    ax2.legend(loc='lower right', bbox_to_anchor=(1, 1), fontsize=10)\n",
    "    plt.draw()\n",
    "    \n",
    "\n",
    "def animate_evaluation(n_pred, pred, relative_pred, labels, ensemble_pred, class_dict, treshold_range, title):\n",
    "        \n",
    "    # create a figure and axes for the animation\n",
    "    fig, (ax1, ax2) = plt.subplots(2, figsize=(15,9))\n",
    "\n",
    "    # define the function that will be called for each frame of the animation\n",
    "    def animate(frame):\n",
    "        \n",
    "        n, p = frame\n",
    "        \n",
    "        ax1.clear(); ax2.clear()\n",
    "        ax1.clear(); ax2.clear()\n",
    "        ax1.set_title(f'Prediction {n+1}', fontsize=16, pad=-5)\n",
    "        \n",
    "        # call function to draw actual trashhold for image_i on figure\n",
    "        draw_evaluation_process(pred[n], \n",
    "                                relative_pred[n], \n",
    "                                labels[n], \n",
    "                                ensemble_pred[n],\n",
    "                                class_dict,\n",
    "                                p, ax1, ax2)\n",
    "\n",
    "    # create a list of tuples for the frames\n",
    "    frames = [(n, p) for n in range(n_pred) for p in treshold_range]\n",
    "\n",
    "    # create the animation\n",
    "    ani = FuncAnimation(fig, animate, frames=frames, repeat=True)\n",
    "\n",
    "    # save the animation as a GIF using Pillow\n",
    "    ani.save(f'./results/{title}.gif', writer=\"pillow\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
