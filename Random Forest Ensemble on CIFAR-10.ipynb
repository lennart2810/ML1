{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329e94ac",
   "metadata": {},
   "source": [
    "*Version: 29.01.2023*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a83d3",
   "metadata": {},
   "source": [
    "<a href=\"https://www.w-hs.de/service/informationen-zur-person/person/nalbach/\">\n",
    "    <img src=\"https://www.w-hs.de/typo3conf/ext/whs/Resources/Public/Images/Pagelayout/w-hs_pagelogo.png\"       alt=\"drawing\" width=\"200\" align=\"right\"/>\n",
    "</a>\n",
    "\n",
    "<h1 align=\"center\">  </h1>\n",
    "<h1 align=\"center\"> Maschine Learning 1 WS 22/23 </h1>\n",
    "<h4 align=\"center\"> Lennart Fuhrig </h4>\n",
    "\n",
    "\n",
    "# Implementation of a Random Forest Ensemble for recognition of classification uncertainty on CIFAR-10 image data.\n",
    "### Subtasks:\n",
    "> * #### Data Preprocessing\n",
    "> * #### Tuning of a Random Forest Base Classifier\n",
    "> * #### Implementation and Training of Ensemble\n",
    "> * #### Ensemble Evaluation\n",
    "\n",
    "<a href=\"https://github.com/lennart2810/ML1\">\n",
    "    <img src=\"https://raw.githubusercontent.com/lennart2810/ML1/master/appendix/bagging.png\"       alt=\"drawing\" width=\"700\" align=\"center\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "302a10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc936926",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_config' from 'tensorflow.python.eager.context' (/Users/lennartfuhrig/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/context.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-64dda9c9b36c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# import ensemble class and functions defined in separate script\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mipynb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfull\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheader_file\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_processed_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMyEnsembleClassifier\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot_MRF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Desktop/Lenny/Uni/Master/3. Semester/ML1/header_file.ipynb\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m    \u001b[0;34m\"outputs\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m    \"source\": [\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0;34m\"import time \\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;34m\"import pprint\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;34m\"import numpy as np\\n\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/keras/backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotobuf\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig_pb2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'get_config' from 'tensorflow.python.eager.context' (/Users/lennartfuhrig/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/context.py)"
     ]
    }
   ],
   "source": [
    "#!pip install ipynb \n",
    "\n",
    "# import ensemble class and functions defined in separate script\n",
    "from ipynb.fs.full.header_file import get_processed_data, MyEnsembleClassifier, plot_MRF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470f6c5",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "> ### `get_processed_data`\n",
    "> uses three nested functions for laoding, preprocessing and splitting the dataset. It takes two optional arguments: `debug` indicates whether to print information during the process or not, whereas the `proportion` input is passed to the `split_train_data` function.\n",
    ">> #### `load_dataset`\n",
    ">> loads the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) using `keras.datasets`. It returns the train and test sets, including images and labels, as well as a dictionary that maps the numeric labels of the classes to their names.\n",
    ">> #### `process_data`\n",
    ">> performs several preprocessing steps on the dataset. It converts the images to grayscale, scales them from 0-255 to 0-1 and flattens the images.\n",
    ">> #### `split_train_data`\n",
    ">> devides the train data into two parts: an *unique* and an *overlapping* set. The `proportion` input is a value between 0 and 1 indicating the proportion of the data to be used as the *overlap* subset.\n",
    "> #### example:\n",
    ">```python\n",
    ">train, unique, overlap, test, class_dict = get_processed_data(debug=True, proportion=0.2)\n",
    ">```\n",
    "A proportion of 20% of the `train` data will be used as the `overlap` subset, remainig samples are stored in the `unique` one. All data outputs are defined as a `list`, each containing a numpy array for the images `(N, 1024)` and labels `(N, )`.\n",
    "\n",
    "> For code use: `get_processed_data??`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4aaf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call preprocessing function\n",
    "train_data, unique_data, overlap_data, test_data, class_dict = get_processed_data(proportion=0.2, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100f2d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print exemplary information for the training data\n",
    "print(f'train data:\\t{len(train_data)}\\t\\t{type(train_data)}')\n",
    "print(f'train images:\\t{train_data[0].shape}\\t{type(train_data[0])}')\n",
    "print(f'train labels:\\t{train_data[1].shape}\\t{type(train_data[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f6cd7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Step 1: Random Forest Base Classifier\n",
    "\n",
    "> The following example code uses [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated) to find the best hyperparameters for a [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) by training and evaluating the `grid_search` object with all possible combinations specified in the `param_grid`. The best set can be extracted to define the **RF Base Classifier**.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {'max_depth': [None, 5, 10], \n",
    "              'n_estimators': [50, 100, 150],\n",
    "              'min_samples_leaf': [1, 2, 5, 10], \n",
    "              'max_features': [\"sqrt\", \"log2\", None]}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid)\n",
    "grid_search.fit(train_images, train_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a8b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random forest base classifier with parameters determined by GridSearchCV\n",
    "def rf_base_clf():\n",
    "    return RandomForestClassifier(n_estimators=100, \n",
    "                                  min_samples_leaf=2, \n",
    "                                  max_features=None, \n",
    "                                  max_depth=None, \n",
    "                                  verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b653b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Step 2: Create and Train Bagging Ensemble\n",
    "\n",
    "> ### `MyEnsembleClassifier`\n",
    "creates an ensemble of classifiers using bagging. It takes three input arguments: `base_classifier`: a function that returns the Random Forest Base Classifier object, `n_classifier`: an integer representing the number of classifiers and `class_dict`: a dictionary containing class labels and descriptions.\n",
    "The class has several methods:\n",
    "> #### `init` \n",
    "initializes the class by creating a list of the specified base classifiers and storing information about the dataset as class attributes.\n",
    "> #### `create_subsets`\n",
    "takes two datasets (`unique` and `overlap`) and an predefined `overlap_dictonary` as inputs. It creates subsets  for each ensemble member to train on by using a combination of sequential sampling from the *unique* subset and random sampling from the *overlapping* one.\n",
    "> #### `train`\n",
    "fits each ensemble member to the subset of data created in the `create_subsets` method. An optional `proportion` parameter can be passed in to reduce the size of the dataset and speed up training during development.\n",
    "> #### `evaluate`\n",
    "takes in two parameters: `images` and `labels`. The method first iterates over each classifier, gets the accuracy scores using the `score` method and means them to an ensemble accuracy. \n",
    "Afterwards it iterates over all classifiers to get their predictions for the input images using the `predict` method. \n",
    "These predictions are stored in an array of shape `N_images, N_classifier` where each element `i,j` is the class predicted by the `classifier_j` for `image_i`.\n",
    "The number of predictions for each class and images is counted and divided by the total number of classifiers to calculate the relative frequencies. \n",
    "The ensemble prediction for each image is determined by the index of the highest value of the relative predictions array, which is also the measure of the certainty of the ensemble prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of ensemble members\n",
    "n_clf = 10\n",
    "\n",
    "# predefined number of samples for the each overlapping subset\n",
    "overlap_dict = {10:6000, 20:5000, 40:3000} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e551f277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the ensemble\n",
    "ensemble = MyEnsembleClassifier(rf_base_clf, n_clf, class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd80b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call method to create subsets\n",
    "ensemble.create_subsets(unique_data, overlap_data, overlap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771ea1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train ensemble\n",
    "ensemble.train(0.1) # train each classifier on 10% of their corresponding subset (default=1 for full training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27b37e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Step 3: Evaluate Ensemble\n",
    "\n",
    "> ### `evaluation`\n",
    "is called by the following `main` function to iterate over all images and check if an ensemble prediction `pred` is subject to uncertainty by comparing the confidence level `o_max` to a `threshold`. If it is uncertain, `M` is incremented and the function checks if `pred` is correct or wrong, incrementing `R` or `F` respectively.\n",
    "Once the loop is finished, the function relativizes the values to the number of images `N`.\n",
    "> ### `main`\n",
    "evaluates the ensemble on a given dataset using the `ensemble.evaluate` method. A list of results for different thresholds in the range of 0 to 1 is created by calling `evaluation` during a list comprehension. Finally, the results are visualized using the `plot_MRF` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce evaluation time if needed\n",
    "develop = True\n",
    "\n",
    "if develop:\n",
    "\n",
    "    samples = 200\n",
    "    test_data[0] = test_data[0][:samples]\n",
    "    test_data[1] = test_data[1][:samples]\n",
    "\n",
    "    train_data[0] = train_data[0][:samples]\n",
    "    train_data[1] = train_data[1][:samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(labels, ensemble_pred, ensemble_pred_confi, treshold):\n",
    "\n",
    "    N, M, R, F = len(labels), 0, 0, 0\n",
    "\n",
    "    # iterate over all images \n",
    "    for label, pred, o_max in zip(labels, ensemble_pred, ensemble_pred_confi):    \n",
    "\n",
    "        # is the ensemble prediction subject to uncertainty? \n",
    "        if o_max < treshold: \n",
    "            M += 1\n",
    "\n",
    "            # is the uncertain prediction right or wrong?\n",
    "            if pred == label: R += 1\n",
    "            else: F += 1\n",
    "                \n",
    "    if M != 0:\n",
    "        # relativize values to N (number of images) (R, F already relative to M)\n",
    "        M = M / N\n",
    "        R = R / N\n",
    "        F = F / N \n",
    "            \n",
    "    return [M, R, F, treshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create horizontal axis\n",
    "treshold_range = (np.round(np.arange(0, 1.1, 0.1), 2)) \n",
    "\n",
    "def main(data, data_name):\n",
    "    \n",
    "    # extract images and labels from given data set\n",
    "    images, labels = data[0], data[1]\n",
    "    \n",
    "    # evaluate ensemble on given dataset\n",
    "    acc, pred, relative_pred, ensemble_pred, ensemble_pred_confi = ensemble.evaluate(images, labels)\n",
    "    \n",
    "    # print ensemble evaluation info\n",
    "    print(f'accuracy:\\t\\t{round(acc, 3)}\\t\\t{type(acc)}')\n",
    "    print(f'predictions:\\t\\t{pred.shape}\\t{type(pred)}')\n",
    "    print(f'relative predictions:\\t{relative_pred.shape}\\t{type(relative_pred)}')\n",
    "    print(f'ensemble prediction:\\t{ensemble_pred.shape}\\t{type(ensemble_pred)}')\n",
    "    print(f'ensemble confidence:\\t{ensemble_pred_confi.shape}\\t{type(ensemble_pred_confi)}')\n",
    "\n",
    "    # evaluation for given thresholds\n",
    "    results = [evaluation(labels, ensemble_pred, ensemble_pred_confi, treshold) for treshold in treshold_range]\n",
    "    \n",
    "    # plot results via plotting function \n",
    "    title = f'{ensemble.clf_n} classifier on {data_name} dataset (accuracy: {round(acc * 100, 3)}%)'\n",
    "    plot_MRF(results, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045be8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate test dataset\n",
    "main(test_data, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate train dataset\n",
    "main(train_data, 'train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
