{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "329e94ac",
   "metadata": {},
   "source": [
    "*Version: 28.01.2023*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "663a83d3",
   "metadata": {},
   "source": [
    "<a href=\"https://www.w-hs.de/service/informationen-zur-person/person/nalbach/\">\n",
    "    <img src=\"https://www.w-hs.de/typo3conf/ext/whs/Resources/Public/Images/Pagelayout/w-hs_pagelogo.png\"       alt=\"drawing\" width=\"200\" align=\"right\"/>\n",
    "</a>\n",
    "\n",
    "<h1 align=\"center\">  </h1>\n",
    "<h1 align=\"center\"> Maschine Learning 1 WS 22/23 </h1>\n",
    "<h4 align=\"center\"> Lennart Fuhrig </h4>\n",
    "\n",
    "\n",
    "# Task\n",
    "> *Aufgabenstellung* hier in ein oder zwei Sätzen beschreiben\n",
    "> * #### Preprocessing\n",
    "> * #### Random Forest Base Classifier\n",
    "> * #### Bagging Ensemble Class\n",
    "> * #### Evaluation\n",
    "\n",
    "<img src=\"./appendix/bagging.png\" alt=\"drawing\" width=\"750\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "302a10f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc936926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enables the import of functions from other notebooks\n",
    "#!pip install ipynb \n",
    "\n",
    "# import ensemble class and functions defined in separate script\n",
    "from ipynb.fs.full.header_file import get_processed_data, MyEnsembleClassifier, plot_MRF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "564d5684",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipynb.fs.full.header_file import animate_evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c470f6c5",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "> #### `get_processed_data`\n",
    "> uses three nested functions for laoding, preprocessing and splitting a dataset to prepare it for use in an ensemble machine learning model. It takes two optional arguments: the `debug` boolean indicates whether to print information during the process or not, whereas the `proportion` input is passed to the `split_train_data` function.\n",
    ">> #### `load_dataset`\n",
    ">> loads the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html) using `keras.datasets`. It returns the train and test data, including images and labels, as well as a dictionary that maps the numeric labels of the classes to their names.\n",
    ">> #### `process_data`\n",
    ">> performs several preprocessing steps on the dataset. It converts the images to grayscale, scales them from 0-255 to 0-1 and flattens the image data as well as the labels.\n",
    ">> #### `split_train_data`\n",
    ">> splits the train data into two parts: an *unique* and an *overlapping* set. The `proportion` input is a value between 0 and 1 indicating the proportion of the data to be used as the *overlap* subset.\n",
    "> #### example:\n",
    ">```python\n",
    ">train, unique, overlap, test, class_dict = get_processed_data(debug=True, proportion=0.2)\n",
    ">```\n",
    "A proportion of 20% of the `train` data will be used as the `overlap` subset, remainig samples are stored in the `unique` one. All data outputs are defined as a `list`, each containing a numpy array for the images `(N, 1024)` and labels `(N, )`.\n",
    "\n",
    "> For **documentation / code** use: `get_processed_data?` / `get_processed_data??`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4c4aaf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. load CIFAR-10 dataset:\n",
      "\n",
      "train:\t(50000, 32, 32, 3), (50000, 1)\n",
      "test:\t(10000, 32, 32, 3), (10000, 1)\n",
      "\n",
      "{0: 'airplane',\n",
      " 1: 'automobile',\n",
      " 2: 'bird',\n",
      " 3: 'cat',\n",
      " 4: 'deer',\n",
      " 5: 'dog',\n",
      " 6: 'frog',\n",
      " 7: 'horse',\n",
      " 8: 'ship',\n",
      " 9: 'truck'}\n",
      "\n",
      "2. preprocess dataset:\n",
      "\n",
      "train:\t(50000, 1024), (50000,)\n",
      "test:\t(10000, 1024), (10000, 1024)\n",
      "\n",
      "3. prepare train data for ensemble:\n",
      "\n",
      "unique:\t(40000, 1024), (40000,)\n",
      "overlap:(10000, 1024), (10000,)\n"
     ]
    }
   ],
   "source": [
    "# call preprocessing function\n",
    "train_data, unique_data, overlap_data, test_data, class_dict = get_processed_data(proportion=0.2, debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "100f2d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data:\t2\t\t<class 'list'>\n",
      "train images:\t(50000, 1024)\t<class 'numpy.ndarray'>\n",
      "train labels:\t(50000,)\t<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# print exemplary information for the training data\n",
    "print(f'train data:\\t{len(train_data)}\\t\\t{type(train_data)}')\n",
    "print(f'train images:\\t{train_data[0].shape}\\t{type(train_data[0])}')\n",
    "print(f'train labels:\\t{train_data[1].shape}\\t{type(train_data[1])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111f6cd7",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Step 1: Random Forest Base Classifier\n",
    "\n",
    "> The following example code uses [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated) to find the best hyperparameters for a [`RandomForestClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) by training and evaluating the `grid_search` object with all possible combinations specified in the `param_grid`. The best set can be extracted to define the **RF Base Classifier**.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {'max_depth': [None, 5, 10], \n",
    "              'n_estimators': [50, 100, 150],\n",
    "              'min_samples_leaf': [1, 2, 5, 10], \n",
    "              'max_features': [\"sqrt\", \"log2\", None]}\n",
    "\n",
    "grid_search = GridSearchCV(estimator=RandomForestClassifier(), param_grid=param_grid)\n",
    "grid_search.fit(train_images, train_labels)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "67a8b32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define random forest base classifier with parameters determined by GridSearchCV\n",
    "def rf_base_clf():\n",
    "    return RandomForestClassifier(n_estimators=100, \n",
    "                                  min_samples_leaf=2, \n",
    "                                  max_features=None, \n",
    "                                  max_depth=None, \n",
    "                                  verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b653b4",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Step 2: Create and Train Bagging Ensemble\n",
    "\n",
    "### `EnsembleClassifier`\n",
    "\n",
    "> #### `init`\n",
    "> * test\n",
    "> * weiterer test\n",
    "\n",
    "> #### `create_subsets`\n",
    "> * test\n",
    "> * weiterer test\n",
    "\n",
    "> #### `train`\n",
    "> * test\n",
    "> * weiterer test\n",
    "\n",
    "> #### `evaluate`\n",
    ">> #### `get_accuracy`\n",
    ">> * test\n",
    "\n",
    ">> #### `get_predictions`\n",
    ">> * test\n",
    "\n",
    "This code defines a class called \"EnsembleClassifier\" that creates an ensemble of classifiers using bagging. The class takes a base classifier, the number of classifiers to create in the ensemble (n_classifier), and a dictionary of classes (class_dict) as inputs.\n",
    "\n",
    "The class has several methods:\n",
    "\n",
    "    init: Initializes the class by creating a list of the specified number of base classifiers and storing the class_dict and class_labels as class attributes.\n",
    "    create_subsets: This method takes two data sets (unique and overlap) and an overlap_dict as inputs, and creates subsets of the data for each ensemble member to train on. It uses a combination of sequential sampling from the unique data set and random sampling from the overlapping data set to create these subsets.\n",
    "    train: This method trains each ensemble member on the subset of data created in the create_subsets method. An optional proportion parameter can be passed in to reduce the size of the data set and speed up training during development.\n",
    "    get_accuracy_score: This method takes images and labels as inputs and calculates the accuracy of each ensemble member on that data.\n",
    "\n",
    "The goal of the code is to create and train an ensemble of classifiers using bagging and to get the confidence of the ensemble on its predictions.\n",
    "\n",
    "\n",
    "Certainly, here are the additional methods that I missed:\n",
    "\n",
    "    evaluate: This method takes images and labels as inputs, and evaluates the ensemble's performance on those images by getting predictions from each ensemble member and returning the accuracy score.\n",
    "    analyse_predictions: This method takes images and labels as inputs, and analyses the ensemble's predictions by getting predictions from each ensemble member, then returning the mean and standard deviation of the predicted class probabilities for each image.\n",
    "\n",
    "The evaluate method uses the get_accuracy_score method to get the accuracy of each ensemble member on the input data, and then return the accuracy score of the ensemble as a whole. The analyse_predictions method uses the predict_proba method to get the class probabilities predicted by each ensemble member, then it returns the mean and standard deviation of the predicted class probabilities for each image. This will give us some idea about the confidence of the ensemble on its predictions, by seeing how close the predictions of the different members are.\n",
    "\n",
    "\n",
    "In der fit Methode trainieren Sie mehrere Instanzen der Basisklasse (z.B. RandomForestClassifier) auf unterschiedlichen Teilmengen der Trainingsdaten und speichern Sie die trainierten Modelle.\n",
    "In der predict Methode rufen Sie die predict Methode für jedes trainierte Modell auf und berechnen die relative Häufigkeit der Vorhersagen. Die Klasse mit der höchsten relative Häufigkeit wird als Vorhersage für die Eingabe zurückgegeben.\n",
    "\n",
    "\n",
    "Documentation for MyEnsembleClassifier:\n",
    "\n",
    "MyEnsembleClassifier is an ensemble classifier that uses bagging to train multiple base classifiers on different subsets of the data. The classifier is initialized with a base classifier, the number of classifiers to be trained, and a dictionary containing class labels.\n",
    "Initialization\n",
    "\n",
    "The class takes three input arguments:\n",
    "\n",
    "    base_classifier: a function that returns a base classifier object. This classifier will be used to train multiple classifiers in the ensemble.\n",
    "\n",
    "    n_classifier: an integer representing the number of classifiers to be trained.\n",
    "\n",
    "    class_dict: a dictionary containing class labels.\n",
    "\n",
    "Upon initialization, the class creates a list of n_classifier base classifiers using list comprehension and a function call to base_classifier(). The class also stores the class_dict and class_y (list of class labels) as class attributes.\n",
    "create_subsets()\n",
    "\n",
    "This method creates subsets of the data that will be used to train the classifiers in the ensemble. The method takes three input arguments:\n",
    "\n",
    "    unique: a tuple containing the unique images and labels of the dataset.\n",
    "\n",
    "    overlap: a tuple containing the images and labels of the dataset that overlap with the unique dataset.\n",
    "\n",
    "    overlap_dict: a dictionary containing the number of samples to be taken from the overlap dataset for each classifier.\n",
    "\n",
    "The method first extracts the images and labels from the unique and overlap datasets. Then, it creates a subset for each classifier by slicing the unique dataset and appending a random sample from the overlap dataset. The subsets are stored as a class attribute.\n",
    "train()\n",
    "\n",
    "This method trains each classifier on their corresponding subset. The method takes one optional input argument, proportion, which is used to reduce the size of the dataset during development to reduce training time. The method prints the training progress of each classifier and the total training duration in seconds.\n",
    "get_accuracy_score()\n",
    "\n",
    "This method calculates the accuracy score for each classifier and the ensemble. The method takes two input arguments:\n",
    "\n",
    "    images: a numpy array containing the images to be evaluated.\n",
    "\n",
    "    labels: a numpy array containing the labels of the images.\n",
    "\n",
    "The method first calculates the accuracy score for each classifier using the score() method and stores the scores in a list. The method then calculates the mean of the scores to get the ensemble accuracy score. The ensemble accuracy score is stored as a class attribute.\n",
    "\n",
    "Note: The code snippet provided is not a complete class definition and may contain errors. It is recommended to test the class with sample data before using it in a production environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2a7a9f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define number of ensemble members\n",
    "n_clf = 20\n",
    "\n",
    "# predefined number of samples for the overlapping subsets (number of samples = f(number of classifier))\n",
    "overlap_dict = {10:6000, 20:5000, 40:3000} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e551f277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initialized ensemble with 20 RandomForestClassifier(max_features=None, min_samples_leaf=2)\n"
     ]
    }
   ],
   "source": [
    "# initialize the ensemble\n",
    "ensemble = MyEnsembleClassifier(rf_base_clf, n_clf, class_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cdd80b31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created 20 subsets, each containing the following:\n",
      "\n",
      "images:\t(7000, 1024)\n",
      "labels:\t(7000,)\n"
     ]
    }
   ],
   "source": [
    "# call method to create subsets\n",
    "ensemble.create_subsets(unique_data, overlap_data, overlap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1771ea1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train each classifier on their corresponding subset:\n",
      "\n",
      "1. classifier\n",
      "2. classifier\n",
      "3. classifier\n",
      "4. classifier\n",
      "5. classifier\n",
      "6. classifier\n",
      "7. classifier\n",
      "8. classifier\n",
      "9. classifier\n",
      "10. classifier\n",
      "11. classifier\n",
      "12. classifier\n",
      "13. classifier\n",
      "14. classifier\n",
      "15. classifier\n"
     ]
    }
   ],
   "source": [
    "# train_ensemble\n",
    "ensemble.train(1) # train each classifier on 2% of their corresponding subset (default = 1 for full training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd27b37e",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Step 3: Evaluate Ensemble\n",
    "\n",
    "### hier evaluation und ensemble.evaluate detallierter beschreiben, wie der plot funktioniert irrelevant\n",
    "\n",
    "This code defines two functions: interpolate_data and plot_results.\n",
    "\n",
    "The interpolate_data function takes two inputs, x and y. It uses the make_interp_spline function from the scipy library to fit a cubic spline to the data points (x, y), and generate a new set of data points with 300 points, where x_new are the new x-coordinates and y_new are the corresponding y-coordinates of the spline. This function returns a list containing x_new and y_new.\n",
    "\n",
    "The plot_results function takes four inputs, data, p_range, title, and relative. data is a list of lists, and each sublist contains 3 values, M, R, and F. p_range is the range of x-coordinates that the data points are plotted along, title is the title of the plot. relative is a boolean indicating whether the values in data are relative or absolute.\n",
    "\n",
    "The function first checks the relative parameter, if it's true it sets the value of M, R, and F by multiplying the values in data by M. Else, the values of M, R, and F are set as the values in data. Then, the function applies the interpolate_data function to the data points for M, R, and F, and assigns the returned values to M_x, M_y, R_x, R_y, F_x, and F_y respectively.\n",
    "Then, the function plots the data points using matplotlib, with options to adjust the style of the plot, and it also sets the title and the labels of the plot. It saves the plot as an image in the results folder, and displays the plot.\n",
    "\n",
    "The goal of this code is to plot the results of some calculations, it interpolates the data points to make the plot smooth, and adjust the plot style, saves the plot as an image, and display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3e38e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for develop\n",
    "\n",
    "develop = False\n",
    "\n",
    "if develop:\n",
    "\n",
    "    samples = 200\n",
    "    test_data[0] = test_data[0][:samples]\n",
    "    test_data[1] = test_data[1][:samples]\n",
    "\n",
    "    samples = 200\n",
    "    train_data[0] = train_data[0][:samples]\n",
    "    train_data[1] = train_data[1][:samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b02b123",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(labels, ensemble_pred, ensemble_pred_confi, treshold):\n",
    "\n",
    "    N, M, R, F = len(labels), 0, 0, 0\n",
    "\n",
    "    # iterate over all images \n",
    "    for label, pred, o_max in zip(labels, ensemble_pred, ensemble_pred_confi):    \n",
    "\n",
    "        # is the ensemble prediction subject to uncertainty? \n",
    "        if o_max < treshold: \n",
    "            M += 1\n",
    "\n",
    "            # is the uncertain prediction right or wrong?\n",
    "            if pred == label: R += 1\n",
    "            else: F += 1\n",
    "                \n",
    "    if M != 0:\n",
    "        # relativize values to N (number of images) (R, F already relative to M)\n",
    "        M = M / N\n",
    "        R = R / N\n",
    "        F = F / N \n",
    "            \n",
    "    return [M, R, F, treshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03df2462",
   "metadata": {},
   "outputs": [],
   "source": [
    "# für endgültige skript main raus, nur ein guten durchlauf zeigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32c5e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create horizontal axis\n",
    "treshold_range = (np.round(np.arange(0, 1.1, 0.1), 2)) \n",
    "\n",
    "def main(data, data_name):\n",
    "    \n",
    "    # extract images and labels from given data set\n",
    "    images, labels = data[0], data[1]\n",
    "    \n",
    "    # evaluate ensemble on given dataset\n",
    "    acc, pred, relative_pred, ensemble_pred, ensemble_pred_confi = ensemble.evaluate(images, labels)\n",
    "    \n",
    "    # print ensemble evaluation info\n",
    "    print(f'accuracy:\\t\\t{round(acc, 3)}\\t\\t{type(acc)}')\n",
    "    print(f'predictions:\\t\\t{pred.shape}\\t{type(pred)}')\n",
    "    print(f'relative predictions:\\t{relative_pred.shape}\\t{type(relative_pred)}')\n",
    "    print(f'ensemble prediction:\\t{ensemble_pred.shape}\\t\\t{type(ensemble_pred)}')\n",
    "    print(f'ensemble confidence:\\t{ensemble_pred_confi.shape}\\t\\t{type(ensemble_pred_confi)}')\n",
    "\n",
    "    # evaluation for given thresholds\n",
    "    results = [evaluation(labels, ensemble_pred, ensemble_pred_confi, treshold) for treshold in treshold_range]\n",
    "    \n",
    "    # plot results via plotting function \n",
    "    title = f'{ensemble.clf_n} classifier on {data_name} dataset (accuracy: {round(acc * 100, 3)}%)'\n",
    "    plot_MRF(results, title)\n",
    "    \n",
    "    # den teil fürs endgültige skript wieder rausschmeißen\n",
    "    n_pred = 10 \n",
    "    title = f'{ensemble.clf_n} classifier'\n",
    "    animate_evaluation(n_pred, pred, relative_pred, labels, ensemble_pred, class_dict, treshold_range, title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045be8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(test_data, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d7167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "main(train_data, 'train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
